### FP8 ì—°ì‚° ì§€ì›ì—¬ë¶€ ì²´í¬ ###
FP8 í•˜ë“œì›¨ì–´ ê°€ì†(Tensor Core)ì€ NVIDIA Ada Lovelace(RTX 40 ì‹œë¦¬ì¦ˆ) ë˜ëŠ” Hopper(H100) ì•„í‚¤í…ì²˜ë¶€í„° ê³µì‹ ì§€ì›í•œë‹¤.
```
import torch

# 1. ì•„í‚¤í…ì²˜ í™•ì¸ (Compute Capability 8.9 ì´ìƒ í•„ìš”)
capability = torch.cuda.get_device_capability()
print(f"GPU Compute Capability: {capability}")

if capability >= (8, 9):
    print("âœ… FP8 í•˜ë“œì›¨ì–´ ê°€ì†ì„ ì§€ì›í•˜ëŠ” GPUì…ë‹ˆë‹¤! (RTX 40xx, H100 ë“±)")
else:
    print("âŒ FP8 ê°€ì†ì€ ì–´ë µì§€ë§Œ, ì†Œí”„íŠ¸ì›¨ì–´ ì—ë®¬ë ˆì´ì…˜ì€ ê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# 2. ì‹¤ì œ FP8 ë°ì´í„° íƒ€ì… ìƒì„± í…ŒìŠ¤íŠ¸
try:
    x = torch.randn(2, 2).to(dtype=torch.float8_e4m3fn, device="cuda")
    print("âœ… FP8(E4M3) í…ì„œ ìƒì„± ì„±ê³µ!")
except Exception as e:
    print(f"âŒ FP8 íƒ€ì… ìƒì„± ì‹¤íŒ¨: {e}")
```

### PTQ with TensorRT Model Optimizer ###





### í—ˆê¹…í˜ì´ìŠ¤ ###
```
from autofp8 import AutoFP8ForCausalLM, BaseQuantizeConfig
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B" # ì›ë³¸ ëª¨ë¸

# 1. í€€íƒ€ì´ì œì´ì…˜ ì„¤ì • (E4M3 í˜•ì‹ ì‚¬ìš©)
# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ì…‹ìœ¼ë¡œ 'ultrachat200k' ë“±ì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
quantize_config = BaseQuantizeConfig(quant_method="fp8", activation_scheme="static")

# 2. ëª¨ë¸ ë¡œë“œ ë° 8ë¹„íŠ¸ ë³€í™˜ (Calibration í¬í•¨)
# ì´ ê³¼ì •ì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ í˜ë ¤ë³´ë‚´ ìŠ¤ì¼€ì¼ë§ íŒ©í„°ë¥¼ êµ¬í•©ë‹ˆë‹¤.
model = AutoFP8ForCausalLM.from_pretrained(model_id, quantize_config)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 3. 8ë¹„íŠ¸ ëª¨ë¸ ì €ì¥ (ê°€ì¤‘ì¹˜ + ìŠ¤ì¼€ì¼ë§ íŒ©í„°ê°€ ì €ì¥ë¨)
save_path = "./Llama-3-8B-FP8"
model.save_quantized(save_path)
print(f"âœ… ëª¨ë¸ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸ (vLLM ì—”ì§„ ì‚¬ìš© ì‹œ ê·¹ê°•ì˜ ì†ë„)
from vllm import LLM, SamplingParams

llm = LLM(model=save_path) # ì €ì¥ëœ FP8 ëª¨ë¸ ë¡œë“œ
output = llm.generate(["í€€íƒ€ì´ì œì´ì…˜ì˜ ì¥ì ì€?"], SamplingParams(temperature=0.7))

print(f"ê²°ê³¼: {output[0].outputs[0].text}")
```
* activation_scheme="static": ìš°ë¦¬ê°€ ê³µë¶€í•œ ì •ì  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°©ì‹ì…ë‹ˆë‹¤. íŒŒì¼ì— ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ë°•ì•„ë„£ìŠµë‹ˆë‹¤.
* AutoFP8: ë‚´ë¶€ì ìœ¼ë¡œ NVIDIA Transformer Engineì„ í™œìš©í•˜ì—¬ H100 ë“±ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë„ë¡ ë³€í™˜í•´ ì¤ë‹ˆë‹¤.
* ì €ì¥ëœ íŒŒì¼: ì €ì¥ í´ë”ë¥¼ ì—´ì–´ë³´ë©´ ê°€ì¤‘ì¹˜ íŒŒì¼ì€ ì¤„ì–´ë“¤ì–´ ìˆê³ , quantize_config.json ë“±ì— ìŠ¤ì¼€ì¼ ì •ë³´ê°€ ê¸°ë¡ëœ ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```
from autofp8 import AutoFP8ForCausalLM, BaseQuantizeConfig
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B"

# 1. ì„¤ì • (ì •ì  ì–‘ìí™”)
quantize_config = BaseQuantizeConfig(quant_method="fp8", activation_scheme="static")

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = AutoFP8ForCausalLM.from_pretrained(model_id, quantize_config)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 3. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ì…‹ ì¤€ë¹„ (ì§ì ‘ ë§Œë“¤ê¸°)
# ì‹¤ì œë¡œëŠ” CNN/DailyMailì´ë‚˜ WikiText ê°™ì€ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
calibration_data = [
    "í€€íƒ€ì´ì œì´ì…˜ì€ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì†ë„ë¥¼ ë†’ì´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "Llama-3 ëª¨ë¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œì„œ ë§¤ìš° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
    "FP8 ì—°ì‚°ì€ ìµœì‹  NVIDIA GPUì—ì„œ ê°•ë ¥í•œ ê°€ì† ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
    # ... ë³´í†µ 128~512ê°œì˜ ìƒ˜í”Œ ë¬¸ì¥ì„ ë„£ìŠµë‹ˆë‹¤.
]

# ë¬¸ì¥ë“¤ì„ í† í°í™”í•˜ì—¬ ëª¨ë¸ì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
examples = []
for text in calibration_data:
    # ëª¨ë¸ì˜ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° ìë¥´ê±°ë‚˜ ì¡°ì ˆí•©ë‹ˆë‹¤.
    tokenized = tokenizer(text, truncation=True, max_length=512, return_tensors="pt")
    examples.append(tokenized)

# 4. í€€íƒ€ì´ì œì´ì…˜ ì‹¤í–‰ (ì´ë•Œ ë°ì´í„°ì…‹ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤!)
# 'examples' íŒŒë¼ë¯¸í„°ê°€ ë°”ë¡œ ìš°ë¦¬ê°€ ê³µë¶€í•œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ì…ë‹ˆë‹¤.
model.quantize(examples=examples)

# 5. ì €ì¥
model.save_quantized("./Llama-3-8B-FP8-Local")
```



### ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ###
* ì¼ë°˜ì ì¸ ëŒ€í™”: HuggingFaceì˜ databricks/databricks-dolly-15k ê°™ì€ ë°ì´í„°ì…‹ í™œìš©.
* íŠ¹ì • ë„ë©”ì¸(ê¸ˆìœµ, ì˜ë£Œ): ì‹¤ì œ ì—…ë¬´ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¬¸ì„œ ìƒ˜í”Œ 200ê°œ ì •ë„ ì¶”ì¶œ.
* ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ íŠ¹ì • ë‹¨ì–´ì—ë§Œ ìµœì í™”(Overfitting)ë˜ì–´ ëª¨ë¸ì´ ë°”ë³´ê°€ ë  ìˆ˜ ìˆë‹¤.

```
from datasets import load_dataset
from transformers import AutoTokenizer
from autofp8 import AutoFP8ForCausalLM, BaseQuantizeConfig

model_id = "meta-llama/Meta-Llama-3-8B"

# 1. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆ: WikiText)
# split="train[:512]"ë¥¼ í†µí•´ ì•ë¶€ë¶„ 512ê°œ ìƒ˜í”Œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¼)
ds = load_dataset("nvidia/wikitext-103", split="train[:512]")

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„
tokenizer = AutoTokenizer.from_pretrained(model_id)
quantize_config = BaseQuantizeConfig(quant_method="fp8", activation_scheme="static")
model = AutoFP8ForCausalLM.from_pretrained(model_id, quantize_config)

# 3. ë°ì´í„°ì…‹ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” 'examples' í˜•íƒœë¡œ ê°€ê³µ
examples = []
for text in ds["text"]:
    # ë¹ˆ ì¤„ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸
    if len(text.strip()) < 10:
        continue
        
    # í† í°í™” (Llama-3ì˜ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° ìë¥´ê¸°)
    encoded = tokenizer(
        text, 
        truncation=True, 
        max_length=512, 
        return_tensors="pt"
    ).to("cuda") # ì—°ì‚°ì„ ìœ„í•´ GPUë¡œ ë³´ëƒ„
    
    examples.append(encoded)

# 4. í€€íƒ€ì´ì œì´ì…˜ ì‹¤í–‰ (ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê³µë¶€í•œ 'ìŠ¤ì¼€ì¼ íŒ©í„°'ê°€ ê³„ì‚°ë¨)
# 512ê°œì˜ ì‹¤ì œ ë¬¸ì¥ì„ í†µê³¼ì‹œí‚¤ë©° ê° ë ˆì´ì–´ì˜ í™œì„±í™” ê°’(Activation) ë¶„í¬ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
print("ğŸš€ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œì‘ (ì´ ê³¼ì •ì—ì„œ ë ˆì´ì–´ë³„ ìµœì  ìŠ¤ì¼€ì¼ì„ ì°¾ìŠµë‹ˆë‹¤)...")
model.quantize(examples=examples)

# 5. ìµœì¢… FP8 ëª¨ë¸ ì €ì¥
model.save_quantized("./Llama-3-8B-FP8-WikiText")
print("âœ… FP8 ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
```

## ì°¸ê³ ìë£Œ ##

* [Optimizing LLMs for Performance and Accuracy with Post-Training Quantization](https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/)

