# infer-on-eks

[1. 설치하기](https://github.com/gnosia93/inference-on-eks/blob/main/lesson/1-install.md)


단순히 특정 모델 하나를 빠르게 서빙하고 싶다면 TRT-LLM의 자체 API나 trtllm-serve만으로 충분합니다. 하지만 멀티 모델 관리, 오토스케일링, 복잡한 큐잉 시스템이 필요하다면 Triton을 함께 사용하는 것이 훨씬 유리합니다

## 레퍼런스 ##
* https://www.index.dev/skill-vs-skill/ai-vllm-vs-tgi-vs-tensorrt-llm 
* https://github.com/NVIDIA/TensorRT-LLM
* https://catalog.workshops.aws/genai-on-eks/en-US
* [TensforRT vs vLLM](https://ncsoft.github.io/ncresearch/512f982a1564b5441d432935a7098146226e20b7)
